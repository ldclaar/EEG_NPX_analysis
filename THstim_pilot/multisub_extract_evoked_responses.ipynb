{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates epoched files for evoked EEG and units responses for multiple subjects. Designed for use with main experimental subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\lesliec\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbd_eeg.tbd_eeg.data_analysis.eegutils import EEGexp\n",
    "from tbd_eeg.tbd_eeg.data_analysis.Utilities.utilities import (\n",
    "    get_stim_events, get_evoked_traces, find_nearest_ind, qualitycheck_trials)\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Developed in NPX_find_bursts_testing.ipynb, this version is faster and only returns start times and spike counts ##\n",
    "## For thalamus ##\n",
    "def find_bursts_THunit(spike_times):\n",
    "    \n",
    "    preISIs = np.diff(spike_times)[:-1]\n",
    "    postISIs = np.diff(spike_times)[1:]\n",
    "    ## Find starts ##\n",
    "    bs_inds = np.nonzero((preISIs > 0.1) * (postISIs < 0.004))[0]\n",
    "    if len(bs_inds) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    burst_starts = bs_inds + 1 # +1 corrects for the actual spike ind\n",
    "    ## Loop through burst starts to find spikes that belong to the burst\n",
    "    burst_counts = []\n",
    "    for st_ind in bs_inds:\n",
    "        spkind = st_ind+1\n",
    "        bcount = 1\n",
    "        while (spkind < len(preISIs)) and (preISIs[spkind] < 0.004):\n",
    "            spkind += 1\n",
    "            bcount += 1\n",
    "        burst_counts.append(bcount)\n",
    "    \n",
    "    return spike_times[burst_starts], np.array(burst_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finds bursts in non-thalamic units ##\n",
    "def find_bursts_otherunit(spike_times):\n",
    "    ISI_threshold = 0.01 # ISI less than or equal to 15 ms\n",
    "    spike_count_thresh = 3 # at least this number of spikes to be considered burst\n",
    "\n",
    "    preISIs = np.insert(np.diff(spike_times), 0, 1.0)\n",
    "    burst_starts = []\n",
    "    burst_counts = []\n",
    "    spkind = 0\n",
    "    while spkind < len(spike_times):\n",
    "        tempevent = [spike_times[spkind]]\n",
    "        spkind += 1\n",
    "        while (spkind < len(spike_times)) and (preISIs[spkind] < ISI_threshold):\n",
    "            tempevent.append(spike_times[spkind])\n",
    "            spkind += 1\n",
    "        if len(tempevent) >= spike_count_thresh:\n",
    "            burst_starts.append(tempevent[0])\n",
    "            burst_counts.append(len(tempevent))\n",
    "        del tempevent\n",
    "    \n",
    "    return np.array(burst_starts), np.array(burst_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_region(sunit_info, struct_tree, annot):\n",
    "    ## Finds a grey matter region above/below an unknown region ##\n",
    "    Vind = sunit_info.CCF_DV\n",
    "    vent_sip = struct_tree.get_structures_by_id([annot[sunit_info.CCF_AP, Vind, sunit_info.CCF_ML]])[0]['structure_id_path']\n",
    "    while not struct_tree.structure_descends_from(vent_sip[-1], 8):\n",
    "        Vind += 1\n",
    "        vent_sip = struct_tree.get_structures_by_id([annot[sunit_info.CCF_AP, Vind, sunit_info.CCF_ML]])[0]['structure_id_path']\n",
    "\n",
    "    Dind = sunit_info.CCF_DV\n",
    "    dors_sip = struct_tree.get_structures_by_id([annot[sunit_info.CCF_AP, Dind, sunit_info.CCF_ML]])[0]['structure_id_path']\n",
    "    while not struct_tree.structure_descends_from(dors_sip[-1], 8):\n",
    "        Dind -= 1\n",
    "        dors_sip = struct_tree.get_structures_by_id([annot[sunit_info.CCF_AP, Dind, sunit_info.CCF_ML]])[0]['structure_id_path']\n",
    "\n",
    "    if (Vind - sunit_info.CCF_DV) <= (sunit_info.CCF_DV - Dind):\n",
    "        return struct_tree.get_structures_by_id([vent_sip[-1]])[0]['acronym']\n",
    "    elif (Vind - sunit_info.CCF_DV) > (sunit_info.CCF_DV - Dind):\n",
    "        return struct_tree.get_structures_by_id([dors_sip[-1]])[0]['acronym']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_from_children(test_id, parent_id, struct_tree):\n",
    "    try:\n",
    "        child_ind = np.nonzero([\n",
    "            struct_tree.structure_descends_from(test_id, x) for x in struct_tree.child_ids([parent_id])[0]\n",
    "        ])[0][0]\n",
    "        return struct_tree.get_structures_by_id([struct_tree.child_ids([parent_id])[0][child_ind]])[0]['acronym']\n",
    "    except:\n",
    "        return struct_tree.get_structures_by_id([parent_id])[0]['acronym']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_region(region_acronym, struct_tree):\n",
    "    # areas_of_interest = {\n",
    "    #     'SM-TH': ['AV', 'CL', 'MD', 'PO', 'PF', 'VAL', 'VPL', 'VPM', 'VM'],\n",
    "    # }\n",
    "    reg_id = struct_tree.get_structures_by_acronym([region_acronym])[0]['id']\n",
    "    if struct_tree.structure_descends_from(reg_id, 567):\n",
    "        if struct_tree.structure_descends_from(reg_id, 315):\n",
    "            return get_region_from_children(reg_id, 315, struct_tree)\n",
    "        elif struct_tree.structure_descends_from(reg_id, 698):\n",
    "            return 'OLF'\n",
    "        elif struct_tree.structure_descends_from(reg_id, 1089):\n",
    "            return get_region_from_children(reg_id, 1089, struct_tree)\n",
    "        elif struct_tree.structure_descends_from(reg_id, 703):\n",
    "            return get_region_from_children(reg_id, 703, struct_tree)\n",
    "        elif struct_tree.structure_descends_from(reg_id, 477):\n",
    "            return 'STR'\n",
    "        elif struct_tree.structure_descends_from(reg_id, 803):\n",
    "            return 'PAL'\n",
    "        else:\n",
    "            return 'unassigned'\n",
    "    elif struct_tree.structure_descends_from(reg_id, 343):\n",
    "        if struct_tree.structure_descends_from(reg_id, 1129):\n",
    "            return 'TH'\n",
    "            # if region_acronym == 'RT':\n",
    "            #     return 'RT-TH'\n",
    "            # elif region_acronym in areas_of_interest['SM-TH']:\n",
    "            #     return 'SM-TH'\n",
    "            # else:\n",
    "            #     return 'other-TH'\n",
    "        elif struct_tree.structure_descends_from(reg_id, 1097):\n",
    "            return 'HY'\n",
    "        else:\n",
    "            return get_region_from_children(reg_id, 343, struct_tree)\n",
    "    else:\n",
    "        return 'unassigned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parent_region_to_df(unit_info_df, struct_tree, annot):\n",
    "    ## First, make sure all names in region column correspond to a CCF region (removes nan values) ##\n",
    "    adj_regions = unit_info_df['region'].values.copy()\n",
    "    for indi, rowi in unit_info_df.iterrows():\n",
    "        try:\n",
    "            str_info = struct_tree.get_structures_by_acronym([rowi.region])[0]\n",
    "        except KeyError:\n",
    "            if rowi.depth <= 0: # unit was placed above brain\n",
    "                new_region_id = annot[rowi.CCF_AP, np.nonzero(annot[rowi.CCF_AP, :, rowi.CCF_ML])[0][0], rowi.CCF_ML]\n",
    "                adj_regions[indi] = struct_tree.get_structures_by_id([new_region_id])[0]['acronym']\n",
    "            else:\n",
    "                Lind = rowi.CCF_ML\n",
    "                while annot[rowi.CCF_AP, rowi.CCF_DV, Lind] == 0:\n",
    "                    Lind -= 1\n",
    "                new_region_id = struct_tree.get_structures_by_id(\n",
    "                    [annot[rowi.CCF_AP, rowi.CCF_DV, Lind]])[0]['structure_id_path'][-1]\n",
    "                adj_regions[indi] = struct_tree.get_structures_by_id([new_region_id])[0]['acronym']\n",
    "    unit_info_df['adj_region'] = adj_regions\n",
    "    \n",
    "    ## Second, re-assign any non-grey matter areas to the closest region ##\n",
    "    adj_regions = unit_info_df['adj_region'].values.copy()\n",
    "    for indi, rowi in unit_info_df.iterrows():\n",
    "        reg_id = struct_tree.get_structures_by_acronym([rowi.adj_region])[0]['id']\n",
    "        if not struct_tree.structure_descends_from(reg_id, 8):\n",
    "            adj_regions[indi] = find_closest_region(rowi, struct_tree, annot)\n",
    "    unit_info_df['adj_region'] = adj_regions\n",
    "    \n",
    "    ## Finally, assign a parent region to each adjusted CCF region ##\n",
    "    parent_regions = unit_info_df['adj_region'].values.copy()\n",
    "    for indi, rowi in unit_info_df.iterrows():\n",
    "        parent_regions[indi] = get_parent_region(rowi.adj_region, struct_tree)\n",
    "    unit_info_df['parent_region'] = parent_regions\n",
    "    \n",
    "    return unit_info_df.drop('adj_region', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load subjects.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mouse</th>\n",
       "      <th>sex</th>\n",
       "      <th>DOB</th>\n",
       "      <th>strain</th>\n",
       "      <th>exp_folder</th>\n",
       "      <th>histology</th>\n",
       "      <th>EEG</th>\n",
       "      <th>stim_tip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mouse771424</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C57BL/6J</td>\n",
       "      <td>THstim_d1_2024-11-14_11-28-16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mouse771424</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C57BL/6J</td>\n",
       "      <td>THstim_d2_2024-11-15_10-51-49</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mouse771425</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C57BL/6J</td>\n",
       "      <td>THstim_d1_2024-11-21_10-59-24</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mouse771425</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C57BL/6J</td>\n",
       "      <td>THstim_d2_2024-11-22_10-49-58</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mouse771426</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C57BL/6J</td>\n",
       "      <td>THstim_d1_2024-12-19_12-19-39</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mouse sex  DOB    strain                     exp_folder  histology  \\\n",
       "0  mouse771424   F  NaN  C57BL/6J  THstim_d1_2024-11-14_11-28-16       True   \n",
       "1  mouse771424   F  NaN  C57BL/6J  THstim_d2_2024-11-15_10-51-49       True   \n",
       "2  mouse771425   F  NaN  C57BL/6J  THstim_d1_2024-11-21_10-59-24       True   \n",
       "3  mouse771425   F  NaN  C57BL/6J  THstim_d2_2024-11-22_10-49-58       True   \n",
       "4  mouse771426   M  NaN  C57BL/6J  THstim_d1_2024-12-19_12-19-39       True   \n",
       "\n",
       "     EEG  stim_tip_distance  \n",
       "0  False                300  \n",
       "1  False                300  \n",
       "2  False                300  \n",
       "3  False                300  \n",
       "4  False                100  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisub_file = r\"C:\\Users\\lesliec\\OneDrive - Allen Institute\\Shared Documents - Lab 328\\Projects\\CL-CM stim\\subject_metadata.csv\"\n",
    "subject_df = pd.read_csv(multisub_file)\n",
    "subject_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing_files = True\n",
    "data_dir = Path(r\"P:\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the running signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse771424: THstim_d1_2024-11-14_11-28-16\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771424: THstim_d2_2024-11-15_10-51-49\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771425: THstim_d1_2024-11-21_10-59-24\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771425: THstim_d2_2024-11-22_10-49-58\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771426: THstim_d1_2024-12-19_12-19-39\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771426: THstim_d2_2024-12-20_09-28-35\n",
      "Experiment type: electrical stimulation\n",
      " Saved running signals.\n",
      "\n",
      "mouse771427: THstim_d1_2025-01-22_10-36-51\n",
      "Experiment type: electrical stimulation\n",
      "mouse771427: THstim_d2_2025-01-23_10-25-10\n",
      "Experiment type: electrical stimulation\n",
      "mouse771427: THstim_d3_2025-01-24_10-40-15\n",
      "Experiment type: electrical and sensory stimulation\n"
     ]
    }
   ],
   "source": [
    "for rowi, exprow in subject_df.iterrows():\n",
    "    print('{}: {}'.format(exprow.mouse, exprow.exp_folder))\n",
    "    data_path = os.path.join(data_dir, exprow.mouse, exprow.exp_folder, 'experiment1', 'recording1')\n",
    "    exp = EEGexp(data_path, preprocess=False, make_stim_csv=False)\n",
    "\n",
    "    ## Set file names ##\n",
    "    running_file = os.path.join(exp.data_folder, 'running_signal.npy')\n",
    "    raw_running_file = os.path.join(exp.data_folder, 'raw_running_signal.npy')\n",
    "    running_ts_file = os.path.join(exp.data_folder, 'running_timestamps_master_clock.npy')\n",
    "\n",
    "    if not os.path.exists(running_ts_file) or overwrite_existing_files:\n",
    "        run_signal, raw_signal, run_timestamps = exp.load_running()\n",
    "        np.save(running_file, run_signal, allow_pickle=False)\n",
    "        np.save(raw_running_file, raw_signal, allow_pickle=False)\n",
    "        np.save(running_ts_file, run_timestamps, allow_pickle=False)\n",
    "        print(' Saved running signals.\\n')\n",
    "    else:\n",
    "        print(' Running signals already exist?\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = MouseConnectivityCache(resolution=25)\n",
    "str_tree = mcc.get_structure_tree()\n",
    "annot, annot_info = mcc.get_annotation_volume()\n",
    "\n",
    "spike_count_threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse771424: THstim_d1_2024-11-14_11-28-16\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeA\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.67 min\n",
      "\n",
      "mouse771424: THstim_d2_2024-11-15_10-51-49\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeA\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.44 min\n",
      "\n",
      "mouse771425: THstim_d1_2024-11-21_10-59-24\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.70 min\n",
      "\n",
      "mouse771425: THstim_d2_2024-11-22_10-49-58\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeA\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.99 min\n",
      "\n",
      "mouse771426: THstim_d1_2024-12-19_12-19-39\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeA\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.30 min\n",
      "\n",
      "mouse771426: THstim_d2_2024-12-20_09-28-35\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeA\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.61 min\n",
      "\n",
      "mouse771427: THstim_d1_2025-01-22_10-36-51\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.48 min\n",
      "\n",
      "mouse771427: THstim_d2_2025-01-23_10-25-10\n",
      "Experiment type: electrical stimulation\n",
      " Getting probe info...\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.36 min\n",
      "\n",
      "mouse771427: THstim_d3_2025-01-24_10-40-15\n",
      "Experiment type: electrical and sensory stimulation\n",
      " Getting probe info...\n",
      "  probeD\n",
      "  probeE\n",
      "  probeF\n",
      "   Time to get unit spike times and save: 0.90 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for eind, exprow in subject_df.iterrows():\n",
    "    print('{}: {}'.format(exprow.mouse, exprow.exp_folder))\n",
    "    data_path = os.path.join(data_dir, exprow.mouse, exprow.exp_folder, 'experiment1', 'recording1')\n",
    "    exp = EEGexp(data_path, preprocess=False, make_stim_csv=False)\n",
    "\n",
    "    probe_list = [x.replace('_sorted', '') for x in exp.experiment_data if 'probe' in x]\n",
    "    if len(probe_list) == 0:\n",
    "        print(' This experiment has no probe data, not making spike times files.\\n')\n",
    "        continue\n",
    "\n",
    "    ## Set file names ##\n",
    "    unit_info_file = os.path.join(exp.data_folder, 'all_units_info.csv')\n",
    "    unit_allspiketimes_file = os.path.join(exp.data_folder, 'units_allspikes.pkl')\n",
    "\n",
    "    if overwrite_existing_files:\n",
    "        pass # will overwrite all subjects' files\n",
    "    else:\n",
    "        if os.path.exists(unit_info_file):\n",
    "            print(' {} already exists, skipping analysis.\\n'.format(unit_info_file))\n",
    "            continue\n",
    "\n",
    "    ## Get probe info ##\n",
    "    print(' Getting probe info...')\n",
    "    start = time.time()\n",
    "    all_units_info = []\n",
    "    unit_allspiketimes = {}\n",
    "    for probe_name in probe_list:\n",
    "        print('  {}'.format(probe_name))\n",
    "        ## Load probe_info.json ##\n",
    "        with open(exp.ephys_params[probe_name]['probe_info']) as data_file:\n",
    "            data = json.load(data_file)\n",
    "        npx_allch = np.array(data['channel'])\n",
    "        surface_ch = int(data['surface_channel'])\n",
    "        allch_z = np.array(data['vertical_pos'])\n",
    "        # ref_mask = np.array(data['mask'])\n",
    "        # npx_chs = np.array([x for x in npx_allch if ref_mask[x] and x <= surface_ch])\n",
    "        probe_ch_depths = allch_z[surface_ch] - allch_z\n",
    "        \n",
    "        ## Load the unit info ##\n",
    "        cluster_group = pd.read_csv(exp.ephys_params[probe_name]['cluster_group'], sep='\\t')\n",
    "        cluster_metrics = pd.read_csv(exp.ephys_params[probe_name]['cluster_metrics'])\n",
    "        spike_clusters = np.load(exp.ephys_params[probe_name]['spike_clusters'])\n",
    "        spike_times = np.load(exp.ephys_params[probe_name]['spike_times'])\n",
    "        \n",
    "        if not np.array_equal(cluster_group['cluster_id'].values.astype('int'), np.unique(spike_clusters)):\n",
    "            print('   IDs from cluster_group.tsv DO NOT match spike_clusters.npy. This may mean there are unsorted units, check in phy.')\n",
    "            continue\n",
    "        if np.array_equal(cluster_group['cluster_id'].values.astype('int'), cluster_metrics['cluster_id'].values.astype('int')):\n",
    "            unit_metrics = pd.merge(cluster_group.rename(columns={'group':'label'}), cluster_metrics, on='cluster_id')\n",
    "        else:\n",
    "            print('   IDs from cluster_group DO NOT match cluster_metrics.')\n",
    "            continue\n",
    "        \n",
    "        ## Select only \"good\" units ##\n",
    "        unit_metrics['spike_count'] = [np.sum(spike_clusters == x) for x in unit_metrics.cluster_id.values]\n",
    "        good_units = unit_metrics[(unit_metrics['label'] == 'good') & (unit_metrics['spike_count'] > spike_count_threshold)]\n",
    "        tempcoords = np.array([[int(y) for y in x.replace('[','').replace(']','').replace(' ','').split(',')] for x in good_units.ccf_coord.values])\n",
    "        \n",
    "        ## Now reorganize metrics to save ##\n",
    "        probe_units = pd.DataFrame([probe_name[-1] + str(x) for x in good_units.cluster_id.values], columns=['unit_id'])\n",
    "        probe_units['probe'] = [probe_name] * len(good_units)\n",
    "        probe_units['peak_ch'] = good_units['peak_channel'].values\n",
    "        probe_units['depth'] = [probe_ch_depths[pkch] for pkch in good_units.peak_channel.values]\n",
    "        probe_units['spike_duration'] = good_units['duration'].values\n",
    "        probe_units['region'] = good_units['area'].values\n",
    "        probe_units['CCF_AP'], probe_units['CCF_DV'], probe_units['CCF_ML'] = tempcoords[:,0], tempcoords[:,1], tempcoords[:,2]\n",
    "        probe_units['firing_rate'] = good_units['firing_rate'].values\n",
    "        probe_units['presence_ratio'] = good_units['presence_ratio'].values\n",
    "        probe_units['isi_viol'] = good_units['isi_viol'].values\n",
    "        probe_units['amplitude_cutoff'] = good_units['amplitude_cutoff'].values\n",
    "        probe_units['spike_count'] = good_units['spike_count'].values\n",
    "        \n",
    "        ## Add parent region column ##\n",
    "        probe_units = add_parent_region_to_df(probe_units, str_tree, annot)\n",
    "        all_units_info.append(probe_units)\n",
    "\n",
    "        for uniti, (uind, unitrow) in zip(good_units.cluster_id.values, probe_units.iterrows()):\n",
    "            spikesi = np.squeeze(spike_times[spike_clusters == uniti])\n",
    "            if unitrow.parent_region == 'TH':\n",
    "                burstsi, burst_counts = find_bursts_THunit(spikesi)\n",
    "            else:\n",
    "                burstsi, burst_counts = find_bursts_otherunit(spikesi)\n",
    "            unit_allspiketimes[unitrow.unit_id] = {\n",
    "                'spikes': spikesi,\n",
    "                'bursts': burstsi,\n",
    "                'burst_counts': burst_counts,\n",
    "            }\n",
    "\n",
    "    ## Now combine all probe units dfs ##\n",
    "    all_units_info_df = pd.concat(all_units_info, ignore_index=True)\n",
    "    all_units_info_df.to_csv(unit_info_file, index=False)\n",
    "    pickle.dump(unit_allspiketimes, open(unit_allspiketimes_file, 'wb'))\n",
    "\n",
    "    end = time.time()\n",
    "    print('   Time to get unit spike times and save: {:.2f} min\\n'.format((end-start)/60))\n",
    "    ## After each subject, delete common variables ##\n",
    "    del probe_units, all_units_info, all_units_info_df, unit_allspiketimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse771427: THstim_d3_2025-01-24_10-40-15\n",
      "Experiment type: electrical and sensory stimulation\n"
     ]
    }
   ],
   "source": [
    "exprow = subject_df.iloc[8]\n",
    "print('{}: {}'.format(exprow.mouse, exprow.exp_folder))\n",
    "data_path = os.path.join(data_dir, exprow.mouse, exprow.exp_folder, 'experiment1', 'recording1')\n",
    "exp = EEGexp(data_path, preprocess=False, make_stim_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['probeD', 'probeE', 'probeF']\n"
     ]
    }
   ],
   "source": [
    "probe_list = [x.replace('_sorted', '') for x in exp.experiment_data if 'probe' in x]\n",
    "print(probe_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_name = 'probeD'\n",
    "spike_count_threshold = 20\n",
    "\n",
    "## Load probe_info.json ##\n",
    "with open(exp.ephys_params[probe_name]['probe_info']) as data_file:\n",
    "    data = json.load(data_file)\n",
    "npx_allch = np.array(data['channel'])\n",
    "surface_ch = int(data['surface_channel'])\n",
    "allch_z = np.array(data['vertical_pos'])\n",
    "ref_mask = np.array(data['mask'])\n",
    "npx_chs = np.array([x for x in npx_allch if ref_mask[x] and x <= surface_ch])\n",
    "probe_ch_depths = allch_z[surface_ch] - allch_z\n",
    "\n",
    "## Load the unit info ##\n",
    "cluster_group = pd.read_csv(exp.ephys_params[probe_name]['cluster_group'], sep='\\t')\n",
    "cluster_metrics = pd.read_csv(exp.ephys_params[probe_name]['cluster_metrics'])\n",
    "spike_clusters = np.load(exp.ephys_params[probe_name]['spike_clusters'])\n",
    "spike_times = np.load(exp.ephys_params[probe_name]['spike_times'])\n",
    "\n",
    "if not np.array_equal(cluster_group['cluster_id'].values.astype('int'), np.unique(spike_clusters)):\n",
    "    print('IDs from cluster_group.tsv DO NOT match spike_clusters.npy. This may mean there are unsorted units, check in phy.')\n",
    "if np.array_equal(cluster_group['cluster_id'].values.astype('int'), cluster_metrics['cluster_id'].values.astype('int')):\n",
    "    unit_metrics = pd.merge(cluster_group.rename(columns={'group':'label'}), cluster_metrics, on='cluster_id')\n",
    "else:\n",
    "    print('IDs from cluster_group DO NOT match cluster_metrics.')\n",
    "\n",
    "## Select only \"good\" units ##\n",
    "unit_metrics['spike_count'] = [np.sum(spike_clusters == x) for x in unit_metrics.cluster_id.values]\n",
    "good_units = unit_metrics[(unit_metrics['label'] == 'good') & (unit_metrics['spike_count'] > spike_count_threshold)]\n",
    "tempcoords = np.array([[int(y) for y in x.replace('[','').replace(']','').replace(' ','').split(',')] for x in good_units.ccf_coord.values])\n",
    "\n",
    "## Now reorganize metrics to save ##\n",
    "probe_units = pd.DataFrame([probe_name[-1] + str(x) for x in good_units.cluster_id.values], columns=['unit_id'])\n",
    "probe_units['probe'] = [probe_name] * len(good_units)\n",
    "probe_units['peak_ch'] = good_units['peak_channel'].values\n",
    "probe_units['depth'] = [probe_ch_depths[pkch] for pkch in good_units.peak_channel.values]\n",
    "probe_units['spike_duration'] = good_units['duration'].values\n",
    "probe_units['region'] = good_units['area'].values\n",
    "probe_units['CCF_AP'], probe_units['CCF_DV'], probe_units['CCF_ML'] = tempcoords[:,0], tempcoords[:,1], tempcoords[:,2]\n",
    "probe_units['firing_rate'] = good_units['firing_rate'].values\n",
    "probe_units['presence_ratio'] = good_units['presence_ratio'].values\n",
    "probe_units['isi_viol'] = good_units['isi_viol'].values\n",
    "probe_units['amplitude_cutoff'] = good_units['amplitude_cutoff'].values\n",
    "probe_units['spike_count'] = good_units['spike_count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>probe</th>\n",
       "      <th>peak_ch</th>\n",
       "      <th>depth</th>\n",
       "      <th>spike_duration</th>\n",
       "      <th>region</th>\n",
       "      <th>CCF_AP</th>\n",
       "      <th>CCF_DV</th>\n",
       "      <th>CCF_ML</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>presence_ratio</th>\n",
       "      <th>isi_viol</th>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <th>spike_count</th>\n",
       "      <th>parent_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D0</td>\n",
       "      <td>probeD</td>\n",
       "      <td>9</td>\n",
       "      <td>3620</td>\n",
       "      <td>0.274707</td>\n",
       "      <td>MRN</td>\n",
       "      <td>329</td>\n",
       "      <td>160</td>\n",
       "      <td>199</td>\n",
       "      <td>1.869871</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.027682</td>\n",
       "      <td>9447.0</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D3</td>\n",
       "      <td>probeD</td>\n",
       "      <td>1</td>\n",
       "      <td>3700</td>\n",
       "      <td>0.247236</td>\n",
       "      <td>MRN</td>\n",
       "      <td>328</td>\n",
       "      <td>163</td>\n",
       "      <td>201</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D7</td>\n",
       "      <td>probeD</td>\n",
       "      <td>1</td>\n",
       "      <td>3700</td>\n",
       "      <td>0.315913</td>\n",
       "      <td>MRN</td>\n",
       "      <td>328</td>\n",
       "      <td>163</td>\n",
       "      <td>201</td>\n",
       "      <td>2.930395</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.172786</td>\n",
       "      <td>0.315145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D10</td>\n",
       "      <td>probeD</td>\n",
       "      <td>1</td>\n",
       "      <td>3700</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>MRN</td>\n",
       "      <td>328</td>\n",
       "      <td>163</td>\n",
       "      <td>201</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D12</td>\n",
       "      <td>probeD</td>\n",
       "      <td>5</td>\n",
       "      <td>3660</td>\n",
       "      <td>0.329648</td>\n",
       "      <td>MRN</td>\n",
       "      <td>329</td>\n",
       "      <td>161</td>\n",
       "      <td>200</td>\n",
       "      <td>6.395605</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.041715</td>\n",
       "      <td>0.157468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unit_id   probe  peak_ch  depth  spike_duration region  CCF_AP  CCF_DV  \\\n",
       "0      D0  probeD        9   3620        0.274707    MRN     329     160   \n",
       "1      D3  probeD        1   3700        0.247236    MRN     328     163   \n",
       "2      D7  probeD        1   3700        0.315913    MRN     328     163   \n",
       "3     D10  probeD        1   3700        0.618090    MRN     328     163   \n",
       "4     D12  probeD        5   3660        0.329648    MRN     329     161   \n",
       "\n",
       "   CCF_ML  firing_rate  presence_ratio  isi_viol  amplitude_cutoff  \\\n",
       "0     199     1.869871            0.37  0.021218          0.027682   \n",
       "1     201     0.007917            0.26  0.000000          0.458100   \n",
       "2     201     2.930395            0.73  0.172786          0.315145   \n",
       "3     201     0.008313            0.27  0.000000          0.500000   \n",
       "4     200     6.395605            0.98  0.041715          0.157468   \n",
       "\n",
       "   spike_count parent_region  \n",
       "0       9447.0            MB  \n",
       "1          NaN            MB  \n",
       "2          NaN            MB  \n",
       "3         40.0            MB  \n",
       "4          NaN            MB  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add parent region column ##\n",
    "probe_units = add_parent_region_to_df(probe_units, str_tree, annot)\n",
    "probe_units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MB' 'RHP' 'VIS']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(probe_units.parent_region.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9447,     40,      6,  14805,     42,  32312, 114746, 116360,\n",
       "        14924,  42674,  17657,    142,     31, 218195,  34801,     12,\n",
       "       136522, 140276, 102046,   6785, 101833,  31264,  21339,  55547,\n",
       "           28, 104193,  23132,     16,  15051,  63218,   4861,     25,\n",
       "       197405,     32,  73147,  47288,      6, 130581,  10987,  28972,\n",
       "          265,    221,  27654, 131489,     51,  95909,     33,     50,\n",
       "            6,     50,  82921,  10385,  12010,   9651,  69487,   9064,\n",
       "        39303,      2,  12663,  34226,  54585,   1633,  47629,  70484,\n",
       "          501,  84338,  71143, 161154,   4169,  28293,  74298,  23758,\n",
       "           13,   7821,  18234,  34639,   3900,      3,   1700,   1718,\n",
       "        89287,  64997,  16507,  12520, 136903,  38629,    326,   3523,\n",
       "         4010,   7453,  41013,  33792,  10475,  32306,  32399,  53756,\n",
       "        21126, 108555,     17,     35,     33, 114043,  12785,  23336,\n",
       "        19825,    115,  14871,  30052,   2449,  16460,   4840,  26515,\n",
       "          841,   3110,   8440,  10467,  22401,  15936,  27000,  20208,\n",
       "          102,   5465,  24760,  35708,  33448,  32053, 106085,  29371,\n",
       "        17603,  38221,  62100,  27085,  41514,  37179,     19,  27134,\n",
       "         1480,  19481,  47430,  18795,  45169,   4479,  25417,  21751,\n",
       "        45934,     15,  92843,  19842,    504,     35,      8,   1931,\n",
       "        48067,  14882,   9663,  51590,  25300,  21769,   1169,  10750,\n",
       "        20763,   2505,  23704,  24059,     28,  25693,   1925,    343,\n",
       "          671,  35915,  37675,  36529,  95378,  19474,  20708,  13584,\n",
       "        34705,  27426,   6636,  17527,    180,    403,  41188, 167935,\n",
       "        14686,  33793,  41072,  30695,  26893,  19670,   1220,  35947,\n",
       "        11650,  26156,  17185,  29922,  15866,  22954,     53,   3285,\n",
       "        19562,  30298,  18784,   3538,   9556,    119,  10791,  19266,\n",
       "        11564,  21716,  53907,  41999,  44090,   1099,  28308,  17034,\n",
       "        21302,   8328,    829,  14197,  24696,  24022,  29369,   2961,\n",
       "        32870,  39050,  28610,  31025,  10593,    251,    751,     83,\n",
       "         7093,    509,   7232,    228,     19,      2,      1,      3,\n",
       "          124,     70,   2322,   3796,  16215,  10400,  11744,   9206,\n",
       "          991,  22165,  17002,  98239,  10655,    183,   1639,  46325,\n",
       "           19,    459,   7774,   6671,     14,     62,   1365,    781,\n",
       "        26667,  42142,  19802,   5543,     17,   7378,   1197,   8858,\n",
       "        17655,  34278,  14698,  36918,  20591,  16234,  10049,  42115,\n",
       "         3594,   1240,  17674,  10219,  20805,   8237,   5322,  15039,\n",
       "         7797,   6529,   8614,  10025,    128,   9558,   7028,   4074,\n",
       "          441,    555,    136,   2191,   7180,  83736,   1446,   7645,\n",
       "          211,  82222,  44768,  14256,   2511,   1026,   8150,  39862,\n",
       "        14504,    247,    234,    358,    571, 140153,   5214,    130,\n",
       "          260,     35,   2012,    522,    908,     43,    979,  61736,\n",
       "        26274,   3938,   4557,   1980,    314,    260,    388,    983,\n",
       "           94,   3475,   7364,  15440,  10886,   1619,    451,    464,\n",
       "          933,    725,    837,   1948,  10027,    203,    475,    272,\n",
       "          249,     13,   8177,  17784,    392,    562,    372])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_units.spike_count.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D523\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "unitrow = probe_units.iloc[238]\n",
    "uniti = good_units.cluster_id.values[238]\n",
    "print(unitrow.unit_id)\n",
    "print(uniti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "diff requires input that is at least one dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_236\\2474038033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mburstsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburst_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_bursts_THunit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspikesi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mburstsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburst_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_bursts_otherunit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspikesi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_236\\3123163567.py\u001b[0m in \u001b[0;36mfind_bursts_otherunit\u001b[1;34m(spike_times)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mspike_count_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;31m# at least this number of spikes to be considered burst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpreISIs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspike_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mburst_starts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mburst_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tbd_eeg\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[0mnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diff requires input that is at least one dimensional\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: diff requires input that is at least one dimensional"
     ]
    }
   ],
   "source": [
    "if unitrow.parent_region == 'TH':\n",
    "    burstsi, burst_counts = find_bursts_THunit(spikesi)\n",
    "else:\n",
    "    burstsi, burst_counts = find_bursts_otherunit(spikesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(burstsi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check trial quality for EEG signals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Re-process quality for all trials in all subjects (updated quality function 6/4/24) ###\n",
    "for mouse, explist in subjects.items():\n",
    "    for exptype, dataloc in explist.items():\n",
    "        print('{}: {}'.format(mouse, exptype))\n",
    "        exp = EEGexp(dataloc, preprocess=False, make_stim_csv=False)\n",
    "        \n",
    "        ## Check quality for all trials ##\n",
    "        if np.any([True for x in exp.experiment_data if 'recording' in x]):\n",
    "            ## Grab exp metadata from Templeton-log_exp ##\n",
    "            exp_meta = metadata[(\n",
    "                (metadata['mouse_name'].str.contains(mouse)) &\n",
    "                (metadata['exp_name'].str.contains(os.path.basename(os.path.dirname(exp.experiment_folder))))\n",
    "            )].squeeze()\n",
    "            badchstr = exp_meta['EEG bad_channels'].replace(' ','')\n",
    "            if badchstr == 'all':\n",
    "                print(' All EEG chs are bad.')\n",
    "                bad_chs = np.arange(30)\n",
    "            else:\n",
    "                bad_chs = []\n",
    "                for char in badchstr.split(','):\n",
    "                    if char.isdecimal():\n",
    "                        bad_chs.append(int(char))\n",
    "            \n",
    "            qualitycheck_trials(exp, known_bad_chs=bad_chs)\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "overwrite_existing_files = False\n",
    "\n",
    "event_window = [-2.0, 2.0]\n",
    "\n",
    "apply_mask = True\n",
    "apply_hpass = True\n",
    "apply_lpass = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process running signal and EEG"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for mouse, explist in subjects.items():\n",
    "    for exptype, dataloc in explist.items():\n",
    "        print('{}: {}'.format(mouse, exptype))\n",
    "        exp = EEGexp(dataloc, preprocess=False, make_stim_csv=False)\n",
    "        \n",
    "        ## Set file names ##\n",
    "        running_file = os.path.join(exp.data_folder, 'running_signal.npy')\n",
    "        running_ts_file = os.path.join(exp.data_folder, 'running_timestamps_master_clock.npy')\n",
    "#         running_ts_file = os.path.join(exp.data_folder, 'running_timestamps.npy')\n",
    "        evoked_folder = os.path.join(exp.data_folder, 'evoked_data')\n",
    "        if not os.path.exists(evoked_folder):\n",
    "            os.mkdir(evoked_folder)\n",
    "        event_running_file = os.path.join(evoked_folder, 'event_running_speed.npy')\n",
    "        event_running_ts_file = os.path.join(evoked_folder, 'event_running_times.npy')\n",
    "        event_EEGtraces_file = os.path.join(evoked_folder, 'event_EEGtraces.npy')\n",
    "        event_EEGtraces_ts_file = os.path.join(evoked_folder, 'event_EEGtraces_times.npy')\n",
    "\n",
    "        ## Load stim log ##\n",
    "        stim_log = pd.read_csv(exp.stimulus_log_file)\n",
    "        all_event_times = stim_log['onset'].values\n",
    "            \n",
    "        ## Load running signal and get mean event speed ##\n",
    "        if not os.path.exists(event_running_file) or overwrite_existing_files:\n",
    "            print('  Getting event-related running...')\n",
    "            if os.path.exists(running_file):\n",
    "                run_signal = np.load(running_file)\n",
    "                run_timestamps = np.load(running_ts_file)\n",
    "            else:\n",
    "                print('  Loading running from sync and saving...')\n",
    "                run_signal, run_timestamps = exp.load_running()\n",
    "                np.save(running_file, run_signal, allow_pickle=False)\n",
    "                np.save(running_ts_file, run_timestamps, allow_pickle=False)\n",
    "            \n",
    "            rinds = np.arange(-int(-event_window[0] * 100), int(event_window[1] * 100))\n",
    "            event_inds = np.array([find_nearest_ind(run_timestamps, x) for x in all_event_times])\n",
    "            event_run_speed = run_signal[np.repeat([rinds], len(event_inds), axis=0).T + event_inds]\n",
    "            event_run_times = rinds / 100\n",
    "            ## Save ##\n",
    "            np.save(event_running_file, event_run_speed, allow_pickle=False)\n",
    "            np.save(event_running_ts_file, event_run_times, allow_pickle=False)\n",
    "            ## Add speed to stim_log ##\n",
    "            evinds = np.nonzero((event_run_times >= -0.5) & (event_run_times < 0.5))[0]\n",
    "            mean_speed = np.mean(event_run_speed[evinds, :], axis=0)\n",
    "            stim_log['mean_speed'] = mean_speed\n",
    "            stim_log['resting_trial'] = stim_log['mean_speed'] == 0\n",
    "            stim_log.to_csv(exp.stimulus_log_file, index=False)\n",
    "            \n",
    "        if np.any([True for x in exp.experiment_data if 'recording' in x]):\n",
    "            ## Grab exp metadata from Templeton-log_exp ##\n",
    "            exp_meta = metadata[(\n",
    "                (metadata['mouse_name'].str.contains(mouse)) &\n",
    "                (metadata['exp_name'].str.contains(os.path.basename(os.path.dirname(exp.experiment_folder))))\n",
    "            )].squeeze()\n",
    "            badchstr = exp_meta['EEG bad_channels'].replace(' ','')\n",
    "            if (not os.path.exists(event_EEGtraces_file) or overwrite_existing_files) and (badchstr != 'all'):\n",
    "                ## Load EEG data and preprocess ##\n",
    "                print('  Loading EEG data...')\n",
    "                datai, tsi = exp.load_eegdata()\n",
    "                eeg_chs = np.arange(0, datai.shape[1])\n",
    "\n",
    "                ## Mask estim artifact ##\n",
    "                if apply_mask:\n",
    "                    mask_samples = int(0.002 * exp.ephys_params['EEG']['sample_rate'])\n",
    "                    for etime in stim_log.loc[stim_log['stim_type'] == 'biphasic', 'onset'].to_numpy():\n",
    "                        val = find_nearest_ind(tsi, etime) - 2\n",
    "                        datai[val:val+mask_samples, :] = datai[val:val-mask_samples:-1, :]\n",
    "\n",
    "                ## Apply high-pass filter ##\n",
    "                if apply_hpass:\n",
    "                    hpb, hpa = signal.butter(3, 0.1/(exp.ephys_params['EEG']['sample_rate']/2), btype='highpass')\n",
    "                    datai = signal.filtfilt(hpb, hpa, datai, axis=0)\n",
    "\n",
    "                ## Get evoked traces ##\n",
    "                print('  Getting EEG traces...')\n",
    "                event_traces, event_ts = get_evoked_traces(\n",
    "                    datai, tsi, all_event_times, -event_window[0], event_window[1], exp.ephys_params['EEG']['sample_rate'])\n",
    "\n",
    "                ## Apply lowpass filter ##\n",
    "                if apply_lpass:\n",
    "                    lpb, lpa = signal.butter(3, 100/(exp.ephys_params['EEG']['sample_rate']/2), btype='low')\n",
    "                    event_traces = signal.filtfilt(lpb, lpa, event_traces, axis=0)\n",
    "\n",
    "                ## Save ##\n",
    "                print('   ...saving {}.'.format(event_EEGtraces_file))\n",
    "                np.save(event_EEGtraces_file, event_traces, allow_pickle=False)\n",
    "                np.save(event_EEGtraces_ts_file, event_ts, allow_pickle=False)\n",
    "            else:\n",
    "                print('  Not creating EEG traces file, it already exists or all EEG chs are bad.')\n",
    "        else:\n",
    "            print('  No EEG in this recording.')\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655955: saline\n",
      "Experiment type: electrical stimulation\n",
      "  Getting probe info...\n",
      "  Getting spike times...\n",
      "  Adding parent region...\n",
      "  Time to get unit spike times and save: 0.99 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mouse, explist in subjects.items():\n",
    "    for exptype, dataloc in explist.items():\n",
    "        print('{}: {}'.format(mouse, exptype))\n",
    "        exp = EEGexp(dataloc, preprocess=False, make_stim_csv=False)\n",
    "        \n",
    "        probe_list = [x.replace('_sorted', '') for x in exp.experiment_data if 'probe' in x]\n",
    "        if len(probe_list) == 0:\n",
    "            print(' This experiment has no probe data, not making spike times files.\\n')\n",
    "            continue\n",
    "        \n",
    "        ## Set file names ##\n",
    "        evoked_folder = os.path.join(exp.data_folder, 'evoked_data')\n",
    "        if not os.path.exists(evoked_folder):\n",
    "            os.mkdir(evoked_folder)\n",
    "        unit_info_file = os.path.join(evoked_folder, 'all_units_info.csv')\n",
    "        unit_allspiketimes_file = os.path.join(evoked_folder, 'units_allspikes.pkl')\n",
    "        unit_eventspikes_file = os.path.join(evoked_folder, 'units_event_spikes.pkl')\n",
    "        if overwrite_existing_files:\n",
    "            pass # will overwrite all subjects' files\n",
    "        else:\n",
    "            if os.path.exists(unit_info_file):\n",
    "                print('  {} already exists, skipping analysis.\\n'.format(unit_info_file))\n",
    "                continue\n",
    "\n",
    "        ## Load stim log ##\n",
    "        stim_log = pd.read_csv(exp.stimulus_log_file)\n",
    "        all_event_times = stim_log['onset'].values\n",
    "        \n",
    "        ## Get probe info ##\n",
    "        print('  Getting probe info...')\n",
    "        probe_data = {}\n",
    "        for pbi, probei in enumerate(probe_list):\n",
    "            probe_data[probei] = {}\n",
    "            ## Load probe_info.json ##\n",
    "            with open(exp.ephys_params[probei]['probe_info']) as data_file:\n",
    "                data = json.load(data_file)\n",
    "            npx_allch = np.array(data['channel'])\n",
    "            surface_ch = int(data['surface_channel'])\n",
    "            allch_z = np.array(data['vertical_pos'])\n",
    "            ref_mask = np.array(data['mask'])\n",
    "            npx_chs = np.array([x for x in npx_allch if ref_mask[x] and x <= surface_ch])\n",
    "            probe_data[probei]['ch_depths'] = allch_z[surface_ch] - allch_z\n",
    "            \n",
    "            ## Select units and get peak chs ##\n",
    "            select_units, peak_chs, unit_metrics = exp.get_probe_units(probei)\n",
    "            ## Sort units ##\n",
    "            probe_data[probei]['units'] = select_units[np.squeeze(np.argsort(peak_chs))]\n",
    "            probe_data[probei]['chs'] = peak_chs[np.squeeze(np.argsort(peak_chs))]\n",
    "            probe_data[probei]['duration'] = unit_metrics.duration.values[np.squeeze(np.argsort(peak_chs))]\n",
    "            \n",
    "            ## Load spike times and cluster ids ##\n",
    "            probe_data[probei]['spike_times'] = np.load(exp.ephys_params[probei]['spike_times'])\n",
    "            probe_data[probei]['spike_clusters'] = np.load(exp.ephys_params[probei]['spike_clusters'])\n",
    "            \n",
    "            if 'area_ch' in data.keys():\n",
    "                probe_data[probei]['areas'] = unit_metrics.area.values[np.squeeze(np.argsort(peak_chs))]\n",
    "                probe_data[probei]['CCF_coords'] = unit_metrics.ccf_coord.values[np.squeeze(np.argsort(peak_chs))]\n",
    "                \n",
    "        ## Get unit info, spikes and event-spikes, then save files ##\n",
    "        print('  Getting spike times...')\n",
    "        start = time.time()\n",
    "        all_units_info = []\n",
    "        unit_allspiketimes = {}\n",
    "        unit_eventspikestimes = {'event_window': event_window, 'event_spikes': {}, 'event_bursts': {}}\n",
    "        for probei, pdata in probe_data.items():\n",
    "            for unitind, uniti in enumerate(pdata['units']):\n",
    "                unit_name = probei[-1] + str(uniti)\n",
    "                spikesi = np.squeeze(pdata['spike_times'][pdata['spike_clusters'] == uniti])\n",
    "                if spikesi.size < 50:\n",
    "                    continue\n",
    "                unit_allspiketimes[unit_name] = {}\n",
    "                \n",
    "                ## Gather unit info ##\n",
    "                if 'areas' in pdata.keys():\n",
    "                    unit_region = pdata['areas'][unitind]\n",
    "                    unit_coords = [\n",
    "                        int(x) for x in pdata['CCF_coords'][unitind].replace('[','').replace(']','').replace(' ','').split(',')\n",
    "                    ]\n",
    "                else:\n",
    "                    unit_region = 'none'\n",
    "                    unit_coords = [-1, -1, -1]\n",
    "                all_units_info.append([\n",
    "                    unit_name, probei, pdata['chs'][unitind], pdata['ch_depths'][pdata['chs'][unitind]],\n",
    "                    pdata['duration'][unitind], unit_region, unit_coords[0], unit_coords[1], unit_coords[2]\n",
    "                ])\n",
    "\n",
    "                ## Get all and event spike times ##\n",
    "                unit_allspiketimes[unit_name]['spikes'] = spikesi\n",
    "                burstsi, burst_counts = find_bursts_indunit(spikesi)\n",
    "                unit_allspiketimes[unit_name]['bursts'] = burstsi\n",
    "                unit_allspiketimes[unit_name]['burst_counts'] = burst_counts\n",
    "                event_raster = []\n",
    "                burst_raster = []\n",
    "                burst_count_raster = []\n",
    "                for eventi in all_event_times:\n",
    "                    spikeinds = np.nonzero((spikesi >= eventi + event_window[0]) & (spikesi <= eventi + event_window[1]))[0]\n",
    "                    event_raster.append(spikesi[spikeinds] - eventi)\n",
    "                    burstinds = np.nonzero((burstsi >= eventi + event_window[0]) & (burstsi <= eventi + event_window[1]))[0]\n",
    "                    burst_raster.append(burstsi[burstinds] - eventi)\n",
    "                    burst_count_raster.append(burst_counts[burstinds])\n",
    "                unit_eventspikestimes['event_spikes'][unit_name] = event_raster\n",
    "                unit_eventspikestimes['event_bursts'][unit_name] = {'times': burst_raster, 'counts': burst_count_raster}\n",
    "\n",
    "        ## Save the data files to mouse's recordingX\\evoked_data folder ##\n",
    "        all_units_info_df = pd.DataFrame(\n",
    "            all_units_info,\n",
    "            columns=['unit_id', 'probe', 'peak_ch', 'depth', 'spike_duration', 'region', 'CCF_AP', 'CCF_DV', 'CCF_ML']\n",
    "        )\n",
    "        \n",
    "        ## Add parent region column ##\n",
    "        if len(np.unique(all_units_info_df['region'].values.astype(str))) > 1:\n",
    "            print('  Adding parent region...')\n",
    "#             sub_CCF_res = subject_df[subject_df['mouse'] == mouse]['CCF_res'].iloc[0]\n",
    "            sub_CCF_res = 25\n",
    "            mcc = MouseConnectivityCache(resolution=sub_CCF_res)\n",
    "            str_tree = mcc.get_structure_tree()\n",
    "            annot, annot_info = mcc.get_annotation_volume()\n",
    "            all_units_info_df = add_parent_region_to_df(all_units_info_df, str_tree, annot)\n",
    "        \n",
    "        all_units_info_df.to_csv(unit_info_file, index=False)\n",
    "        pickle.dump(unit_allspiketimes, open(unit_allspiketimes_file, 'wb'))\n",
    "        pickle.dump(unit_eventspikestimes, open(unit_eventspikes_file, 'wb'))\n",
    "\n",
    "        end = time.time()\n",
    "        print('  Time to get unit spike times and save: {:.2f} min\\n'.format((end-start)/60))\n",
    "        ## After each subject, delete common variables ##\n",
    "        del stim_log, probe_data, all_event_times, all_units_info, all_units_info_df, unit_allspiketimes, unit_eventspikestimes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbd_eeg",
   "language": "python",
   "name": "tbd_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
