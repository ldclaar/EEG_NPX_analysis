{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import gspread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import integrate, signal, fftpack, stats, ndimage\n",
    "import statsmodels.stats.multitest as multitest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\lesliec\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbd_eeg.tbd_eeg.data_analysis.eegutils import EEGexp\n",
    "from tbd_eeg.tbd_eeg.data_analysis.Utilities.utilities import (\n",
    "    get_stim_events,\n",
    "    get_evoked_traces,\n",
    "    get_evoked_firing_rates,\n",
    "    find_nearest_ind\n",
    ")\n",
    "from allensdk.brain_observatory.ecephys.lfp_subsampling.subsampling import subsample_lfp, remove_lfp_offset\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CCF for identifying cortical areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = MouseConnectivityCache(resolution=10)\n",
    "str_tree = mcc.get_structure_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Zap_Zip-log_exp to get metadata for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gc = gspread.service_account() # need a key file to access the account\n",
    "_sh = _gc.open('Zap_Zip-log_exp') # open the spreadsheet\n",
    "_df = pd.DataFrame(_sh.sheet1.get()) # load the first worksheet\n",
    "zzmetadata = _df.T.set_index(0).T # put it in a nicely formatted dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define areas of interest to plot population activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_of_interest = {\n",
    "    'MO': [\n",
    "        'MOp1', 'MOp2/3', 'MOp5', 'MOp6a', 'MOp6b',\n",
    "        'MOs1', 'MOs2/3', 'MOs5', 'MOs6a', 'MOs6b'\n",
    "    ],\n",
    "    'ACA': [\n",
    "        'ACAd1', 'ACAd2/3', 'ACAd5', 'ACAd6a', 'ACAd6b',\n",
    "        'ACAv1', 'ACAv2/3', 'ACAv5', 'ACAv6a', 'ACAv6b'\n",
    "    ],\n",
    "    'SS': [\n",
    "        'SSp-bfd1', 'SSp-bfd2/3', 'SSp-bfd4', 'SSp-bfd5', 'SSp-bfd6a', 'SSp-bfd6b',\n",
    "        'SSp-ll1', 'SSp-ll2/3', 'SSp-ll4', 'SSp-ll5', 'SSp-ll6a', 'SSp-ll6b',\n",
    "        'SSp-tr1', 'SSp-tr2/3', 'SSp-tr4', 'SSp-tr5', 'SSp-tr6a', 'SSp-tr6b'\n",
    "    ],\n",
    "    'VIS': [\n",
    "        'VISp1', 'VISp2/3', 'VISp4', 'VISp5', 'VISp6a', 'VISp6b',\n",
    "        'VISam1', 'VISam2/3', 'VISam4', 'VISam5', 'VISam6a', 'VISam6b',\n",
    "        'VISpm1', 'VISpm2/3', 'VISpm4', 'VISpm5', 'VISpm6a', 'VISpm6b',\n",
    "        'VISrl1', 'VISrl2/3', 'VISrl4', 'VISrl5', 'VISrl6a', 'VISrl6b',\n",
    "    ],\n",
    "    'MO-TH': [\n",
    "        'AV', 'CL', 'MD', 'PO', 'RT', 'VAL', 'VPL', 'VPM', 'VM'\n",
    "    ],\n",
    "}\n",
    "\n",
    "area_colors = {\n",
    "    'MO': 'blue',\n",
    "    'ACA': 'deepskyblue',\n",
    "    'SS': 'blueviolet',\n",
    "    'VIS': 'green',\n",
    "    'MO-TH': 'steelblue',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_colors = {\n",
    "    'awake': (120/255, 156/255, 74/255),\n",
    "    'anesthetized': (130/255, 122/255, 163/255),\n",
    "    'recovery': (93/255, 167/255, 229/255)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Simone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutocleanLFP(InputData):\n",
    "    PreviousIndexMax = -1\n",
    "    WhileOn = 1\n",
    "    while WhileOn:\n",
    "        if InputData.ndim == 3:\n",
    "            DataEvoked = np.mean(InputData, axis=2)\n",
    "        else:\n",
    "            DataEvoked = InputData\n",
    "\n",
    "        AbsDiff = np.abs(np.diff(DataEvoked[:,:],2))\n",
    "        avg = (AbsDiff[:,2:] + AbsDiff[:,0:-2]) / 2\n",
    "        DevianceMatrix = (AbsDiff[:,1:-1] - avg)\n",
    "        DevianceVector = np.mean(abs(DevianceMatrix), axis=0)\n",
    "\n",
    "        StdTh = np.std(DevianceVector)*7\n",
    "        index_max = max(range(len(DevianceVector)), key=DevianceVector.__getitem__)\n",
    "        \n",
    "        if not (index_max == PreviousIndexMax):\n",
    "            PreviousIndexMax = index_max\n",
    "            if DevianceVector[index_max] > StdTh:\n",
    "                if InputData.ndim == 3:\n",
    "                    InputData[:,index_max+2,:] = (InputData[:,index_max+2+1,:] + InputData[:,index_max+2-1,:]) / 2\n",
    "                else:\n",
    "                    InputData[:,index_max+2] = (InputData[:,index_max+2+1] + InputData[:,index_max+2-1]) / 2\n",
    "#                 print(DevianceVector[index_max] / StdTh)\n",
    "            else:\n",
    "                WhileOn = 0\n",
    "        else:\n",
    "            WhileOn = 0\n",
    "    return InputData\n",
    "\n",
    "def moving_average_2dim(x, w):\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, np.ones(w), 'valid'), axis=0, arr=x) / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCSD(LFPdata, LFPts, baseline_range=[-2, -0.01]):\n",
    "    BLinds = [find_nearest_ind(LFPts, baseline_range[0]), find_nearest_ind(LFPts, baseline_range[1])]\n",
    "    tempdata = LFPdata\n",
    "    \n",
    "    ## Create buffer above and below the target area's chs for spatial smoothing ##\n",
    "    ## Do this for the whole probe, not for each area ##\n",
    "    tmpBoundaryStart = tempdata[:,0,:]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[:,np.newaxis,:], 14, axis=1)\n",
    "    tmpBoundaryEnd = tempdata[:,-1,:]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[:,np.newaxis,:], 14, axis=1)\n",
    "    tempdata = np.concatenate((BoundaryStart, tempdata, BoundaryEnd), axis=1)\n",
    "    \n",
    "    ## Baseline correct the LFP ##\n",
    "    tmpBaselineCorr = np.mean(tempdata[BLinds[0]:BLinds[1],:,:], axis=0)\n",
    "    tempdata = tempdata - tmpBaselineCorr[np.newaxis,:,:]\n",
    "    \n",
    "    ## Create buffer before and after the epoch window for temporal smoothing ##\n",
    "    tmpBoundaryStart = tempdata[0,:,:]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[np.newaxis,:,:], 2, axis=0)\n",
    "    tmpBoundaryEnd = tempdata[-1,:,:]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[np.newaxis,:,:], 1, axis=0)\n",
    "    tempdata = np.concatenate((BoundaryStart, tempdata, BoundaryEnd), axis=0)\n",
    "    \n",
    "    ## Remove bad LFP chs and interpolate data ##\n",
    "    tempdata = AutocleanLFP(tempdata)\n",
    "    \n",
    "    ## Perform spatial and temporal smoothing ##\n",
    "    ## This should output an array with original LFP dimensions + 2 extra \"chs\" ##\n",
    "    smoothdata = np.zeros((LFPdata.shape[0], LFPdata.shape[1]+2, LFPdata.shape[2]))\n",
    "    for iTrial in range(0, tempdata.shape[2]):\n",
    "        tmpMatrix = tempdata[:,:,iTrial]\n",
    "        tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 24) # space, corrects for time mismatch due to multiplexing\n",
    "        tmpMatrix = moving_average_2dim(tmpMatrix, 4) # space, corrects for non-linear ch arrangement\n",
    "        tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 4) # temporal smoothing\n",
    "        smoothdata[:,:,iTrial] = tmpMatrix\n",
    "    del tempdata\n",
    "        \n",
    "    ## 2nd derivative across chs ##\n",
    "    CSDdata = np.diff(smoothdata, n=2, axis=1)\n",
    "    del smoothdata\n",
    "    \n",
    "    ## Another baseline correction ##\n",
    "    tmpBaselineCorr = np.mean(CSDdata[BLinds[0]:BLinds[1],:,:], axis=0)\n",
    "    CSDdata = CSDdata - tmpBaselineCorr[np.newaxis,:,:]\n",
    "    \n",
    "    return CSDdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load subjects from file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(r'C:\\Users\\lesliec\\OneDrive - Allen Institute\\data\\all_EEG_subjects.json') as subjects_file:\n",
    "    multi_sub_dict = json.load(subjects_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load fewer subjects for testing script ##\n",
    "with open(r'C:\\Users\\lesliec\\OneDrive - Allen Institute\\data\\TEST_probe_subjects.json') as subjects_file:\n",
    "    multi_sub_dict = json.load(subjects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EEG_exp and gather LFP traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_existing_files = False\n",
    "\n",
    "before_event = 2. # s, look at 2 s pre-stim\n",
    "after_event = 2. # s, look at 2 s post-stim\n",
    "\n",
    "apply_mask = True\n",
    "subsampling_factor = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569073a\n",
      "Experiment type: electrical and sensory stimulation\n",
      " estim_vis\n",
      " Getting probe info...\n",
      " Extracting traces...\n",
      "  probeB - time to mask artifact: 0.22 min\n",
      "  probeB - time to downsample: 6.03 min\n",
      "  probeB - time to epoch LFP: 0.82 min\n",
      "  probeB - starting CSD analysis...\n",
      "(2000, 384, 2040)\n",
      "  probeB - time to analyze CSD: 4.70 min\n",
      "  probeC - time to mask artifact: 0.92 min\n",
      "  probeC - time to downsample: 6.06 min\n",
      "  probeC - time to epoch LFP: 0.82 min\n",
      "  probeC - starting CSD analysis...\n",
      "(2000, 384, 2040)\n",
      "  probeC - time to analyze CSD: 4.77 min\n",
      "  probeF - time to mask artifact: 0.92 min\n",
      "  probeF - time to downsample: 6.08 min\n",
      "  probeF - time to epoch LFP: 0.82 min\n",
      "  probeF - starting CSD analysis...\n",
      "(2000, 384, 2040)\n",
      "  probeF - time to analyze CSD: 5.02 min\n",
      " Saving traces...\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\MO_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\MO_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\ACA_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\ACA_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\SS_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\SS_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\VIS_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\VIS_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\MO-TH_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\MO-TH_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\evokedCSDinfo.json.\n",
      "  Saving F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1\\evoked_data\\evokedCSDtimestamps.npy.\n",
      "\n",
      "571619b\n",
      "Experiment type: electrical stimulation\n",
      " estim2\n",
      " Getting probe info...\n",
      " Extracting traces...\n",
      "  probeB - time to mask artifact: 0.69 min\n",
      "  probeB - time to downsample: 4.41 min\n",
      "  probeB - time to epoch LFP: 0.66 min\n",
      "  probeB - starting CSD analysis...\n",
      "(2000, 384, 1440)\n",
      "  probeB - time to analyze CSD: 3.69 min\n",
      "  probeC - time to mask artifact: 0.66 min\n",
      "  probeC - time to downsample: 4.77 min\n",
      "  probeC - time to epoch LFP: 0.65 min\n",
      "  probeC - starting CSD analysis...\n",
      "(2000, 384, 1440)\n",
      "  probeC - time to analyze CSD: 3.78 min\n",
      "  probeF - time to mask artifact: 0.66 min\n",
      "  probeF - time to downsample: 4.84 min\n",
      "  probeF - time to epoch LFP: 0.66 min\n",
      "  probeF - starting CSD analysis...\n",
      "(2000, 384, 1440)\n",
      "  probeF - time to analyze CSD: 3.41 min\n",
      " Saving traces...\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\MO_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\MO_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\ACA_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\ACA_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\SS_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\SS_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\VIS_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\VIS_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\MO-TH_evokedLFPtraces_CSD.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\MO-TH_evokedCSDtraces.npy.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\evokedCSDinfo.json.\n",
      "  Saving F:\\EEG_exp\\mouse571619\\estim2_2021-03-19_10-09-01\\experiment1\\recording1\\evoked_data\\evokedCSDtimestamps.npy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mouse_num, rec_folder in multi_sub_dict.items():\n",
    "    print('{}'.format(mouse_num))\n",
    "    exp = EEGexp(rec_folder, preprocess=False, make_stim_csv=False)\n",
    "    exp_tag = exp.experiment_folder[exp.experiment_folder.find('mouse')+12:exp.experiment_folder.find(str(exp.date.year))-1]\n",
    "    print(' {}'.format(exp_tag))\n",
    "    \n",
    "    probe_list = [x.replace('_sorted', '') for x in exp.experiment_data if 'probe' in x]\n",
    "    if len(probe_list) == 0:\n",
    "        print(' This experiment has no probe data, not making CSD files.\\n')\n",
    "        continue\n",
    "    \n",
    "    ## Set file names ##\n",
    "    evoked_folder = os.path.join(exp.data_folder, 'evoked_data')\n",
    "    if not os.path.exists(evoked_folder):\n",
    "        os.mkdir(evoked_folder)\n",
    "    CSDinfo_filename = os.path.join(evoked_folder, 'evokedCSDinfo.json')\n",
    "    CSDtime_filename = os.path.join(evoked_folder, 'evokedCSDtimestamps.npy')\n",
    "    if os.path.exists(CSDinfo_filename) and not overwrite_existing_files:\n",
    "        print(' CSD traces already exist, not overwriting files.\\n')\n",
    "        continue\n",
    "    \n",
    "    ## Load stim log ##\n",
    "    stim_log = pd.read_csv(exp.stimulus_log_file)\n",
    "    stim_log = stim_log.astype({'parameter': str})\n",
    "    all_event_times = stim_log['onset'].values\n",
    "    \n",
    "    ## Get probe info ##\n",
    "    print(' Getting probe info...')\n",
    "    probe_LFP_data = {}\n",
    "    probe_locs = np.ones((len(probe_list)), dtype=bool)\n",
    "    for pbi, probei in enumerate(probe_list):\n",
    "        ## Load probe_info.json ##\n",
    "        with open(exp.ephys_params[probei]['probe_info']) as data_file:\n",
    "            data = json.load(data_file)\n",
    "        if 'area_ch' in data.keys():\n",
    "            probe_LFP_data[probei] = {}\n",
    "            ## Get info ##\n",
    "            npx_allch = np.array(data['channel']) # this is an array from 0 to 384\n",
    "            surface_ch = int(data['surface_channel']) # the electrode we said was at the brain surface\n",
    "            air_ch = int(data['air_channel']) # the electrode at the ACSF/air border\n",
    "            allch_z = np.array(data['vertical_pos']) # vertical pos of each elec (um), rel to the tip\n",
    "            ref_mask = np.array(data['mask']) # contains a False for Npx reference channels and \"bad chs\"\n",
    "            ## Get all ch depths ##\n",
    "            npx_chs = np.array([x for x in npx_allch if ref_mask[x] and x <= surface_ch])\n",
    "            probe_LFP_data[probei]['ch_depths'] = allch_z[surface_ch] - allch_z\n",
    "            probe_LFP_data[probei]['all_chs'] = npx_allch\n",
    "            ## Get saline chs for re-ref ##\n",
    "            if air_ch - 10 > surface_ch:\n",
    "                probe_LFP_data[probei]['saline_chs'] = np.arange(air_ch - 10, air_ch)\n",
    "            else:\n",
    "                probe_LFP_data[probei]['saline_chs'] = np.arange(surface_ch + 1, air_ch)\n",
    "            ## Save info ##\n",
    "            probe_LFP_data[probei]['ch_areas'] = np.array(data['area_ch'])\n",
    "            probe_LFP_data[probei]['pop_chs'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "            probe_LFP_data[probei]['pop_ch_depths'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "            probe_LFP_data[probei]['pop_ch_areas'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "            probe_LFP_data[probei]['pop_ch_mask'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "            for chind in npx_allch: ## Now this will look at all chs, not excluding ACSF or bad chs ##\n",
    "                parent_region = [\n",
    "                    k for k in list(areas_of_interest.keys()) if probe_LFP_data[probei]['ch_areas'][chind] in areas_of_interest[k]\n",
    "                ]\n",
    "                if len(parent_region) == 1:\n",
    "                    probe_LFP_data[probei]['pop_chs'][parent_region[0]].append(chind)\n",
    "                    probe_LFP_data[probei]['pop_ch_depths'][parent_region[0]].append(probe_LFP_data[probei]['ch_depths'][chind])\n",
    "                    probe_LFP_data[probei]['pop_ch_areas'][parent_region[0]].append(probe_LFP_data[probei]['ch_areas'][chind])\n",
    "                    probe_LFP_data[probei]['pop_ch_mask'][parent_region[0]].append(ref_mask[chind])\n",
    "        else:\n",
    "            print('  {} does not have area assignments, not processing.'.format(probei))\n",
    "            probe_locs[pbi] = False\n",
    "            \n",
    "    if probe_locs.any():\n",
    "        pass\n",
    "    else:\n",
    "        print(' NO area assignments for any probes, not analyzing.\\n')\n",
    "        continue\n",
    "            \n",
    "    ## Extract LFP traces for each probe ##\n",
    "    print(' Extracting traces...')\n",
    "    pop_LFP = {key: [] for key in list(areas_of_interest.keys())}\n",
    "    pop_CSD = {key: [] for key in list(areas_of_interest.keys())}\n",
    "    for probei, LFP_data in probe_LFP_data.items():\n",
    "        ## Memmap LFP data ##\n",
    "        start = time.time()\n",
    "        lfp_ts = np.load(exp.ephys_params[probei]['lfp_timestamps'])\n",
    "        lfp_mm = np.memmap(\n",
    "            exp.ephys_params[probei]['lfp_continuous'], dtype='int16',\n",
    "            shape=(lfp_ts.size, exp.ephys_params[probei]['num_chs']), mode='r')\n",
    "        ## Mask artifact ##\n",
    "        if apply_mask:\n",
    "            mask_samples = int(0.002 * exp.ephys_params[probei]['lfp_sample_rate'])\n",
    "            lfpdata = lfp_mm.copy()\n",
    "            for etime in stim_log.loc[stim_log['stim_type'] == 'biphasic', 'onset'].to_numpy():\n",
    "                val = find_nearest_ind(lfp_ts, etime)\n",
    "                lfpdata[val:val+mask_samples, :] = lfpdata[val:val-mask_samples:-1, :]\n",
    "            end = time.time()\n",
    "            print('  {} - time to mask artifact: {:.2f} min'.format(probei, (end-start)/60))\n",
    "        else:\n",
    "            lfpdata = lfp_mm.copy()\n",
    "        del lfp_mm\n",
    "        ## Downsample timestamps and data ##\n",
    "        start = time.time()\n",
    "        lfp_ts = lfp_ts[::subsampling_factor]\n",
    "        new_samp_rate = exp.ephys_params[probei]['lfp_sample_rate'] / subsampling_factor\n",
    "        lfpdata = subsample_lfp(lfpdata, LFP_data['all_chs'], subsampling_factor)\n",
    "        end = time.time()\n",
    "        print('  {} - time to downsample: {:.2f} min'.format(probei, (end-start)/60))\n",
    "        ## Remove DC offset ##\n",
    "        lfpdata = remove_lfp_offset(lfpdata, new_samp_rate, 0.1, 1)\n",
    "        ## Subtract median of ACSF chs ##\n",
    "        clean_lfp = np.zeros(lfpdata.shape, dtype='int16')\n",
    "        saline_ref = np.median(lfpdata[:, LFP_data['saline_chs']], axis=1)\n",
    "        for chi in LFP_data['all_chs']:\n",
    "            tmp = lfpdata[:, chi] - saline_ref\n",
    "            clean_lfp[:, chi] = tmp.astype('int16')\n",
    "        del lfpdata\n",
    "        ## Get evoked LFP traces ##\n",
    "        start = time.time()\n",
    "        lfp_event_traces, lfp_event_ts = get_evoked_traces(\n",
    "            clean_lfp, lfp_ts, stim_log['onset'].values, before_event, after_event, new_samp_rate)\n",
    "        lfp_event_traces = lfp_event_traces * exp.ephys_params[probei]['bit_volts']\n",
    "        del lfp_ts, clean_lfp\n",
    "        end = time.time()\n",
    "        print('  {} - time to epoch LFP: {:.2f} min'.format(probei, (end-start)/60))\n",
    "        \n",
    "        ## Analyze CSD ##\n",
    "        print('  {} - starting CSD analysis...'.format(probei))\n",
    "        start = time.time()\n",
    "        csd_event_traces = computeCSD(lfp_event_traces, lfp_event_ts)\n",
    "        print(csd_event_traces.shape)\n",
    "        end = time.time()\n",
    "        print('  {} - time to analyze CSD: {:.2f} min'.format(probei, (end-start)/60))\n",
    "        ## Store LFP and CSD event traces ##\n",
    "        for region in LFP_data['pop_chs'].keys():\n",
    "            pop_chs = LFP_data['pop_chs'][region]\n",
    "            if len(pop_chs) == 0:\n",
    "                continue\n",
    "            pop_LFP[region].append(lfp_event_traces[:, pop_chs, :])\n",
    "            pop_CSD[region].append(csd_event_traces[:, pop_chs, :])\n",
    "        del lfp_event_traces, csd_event_traces\n",
    "        \n",
    "    ## Reorganize data ##\n",
    "    pop_ch_depths = {key: [] for key in list(areas_of_interest.keys())}\n",
    "    pop_ch_areas = {key: [] for key in list(areas_of_interest.keys())}\n",
    "    pop_ch_mask = {key: [] for key in list(areas_of_interest.keys())}\n",
    "    for probei in probe_LFP_data.keys():\n",
    "        for region in probe_LFP_data[probei]['pop_ch_depths'].keys():\n",
    "            pop_ch_depths[region].append(probe_LFP_data[probei]['pop_ch_depths'][region])\n",
    "            pop_ch_areas[region].append(probe_LFP_data[probei]['pop_ch_areas'][region])\n",
    "            pop_ch_mask[region].append(probe_LFP_data[probei]['pop_ch_mask'][region])\n",
    "\n",
    "    ## Save the data ##\n",
    "    print(' Saving traces...')\n",
    "    LFPtraces_info = {}\n",
    "    for region in pop_LFP.keys():\n",
    "        LFP_filename = os.path.join(evoked_folder, region + '_evokedLFPtraces_CSD.npy')\n",
    "        CSD_filename = os.path.join(evoked_folder, region + '_evokedCSDtraces.npy')\n",
    "        if len(pop_LFP[region]) == 0:\n",
    "            print('  No chs were found for {}, not saving data.'.format(region))\n",
    "            continue\n",
    "        LFPtraces_info[region] = {}\n",
    "\n",
    "        reg_ch_depths = np.concatenate(pop_ch_depths[region])\n",
    "        reg_ch_areas = np.concatenate(pop_ch_areas[region])\n",
    "        reg_ch_mask = np.concatenate(pop_ch_mask[region])\n",
    "        LFP_traces = np.concatenate(pop_LFP[region], axis=1)\n",
    "        CSD_traces = np.concatenate(pop_CSD[region], axis=1)\n",
    "\n",
    "        CH_DEPTH_SORT = np.argsort(reg_ch_depths)\n",
    "        LFPtraces_info[region]['ch_depths'] = reg_ch_depths[CH_DEPTH_SORT].tolist()\n",
    "        LFPtraces_info[region]['ch_areas'] = reg_ch_areas[CH_DEPTH_SORT].tolist()\n",
    "        LFPtraces_info[region]['ch_mask'] = reg_ch_mask[CH_DEPTH_SORT].tolist()\n",
    "\n",
    "        print('  Saving {}.'.format(LFP_filename))\n",
    "        np.save(LFP_filename, LFP_traces[:, CH_DEPTH_SORT, :], allow_pickle=False)\n",
    "        print('  Saving {}.'.format(CSD_filename))\n",
    "        np.save(CSD_filename, CSD_traces[:, CH_DEPTH_SORT, :], allow_pickle=False)\n",
    "\n",
    "    print('  Saving {}.'.format(CSDinfo_filename))\n",
    "    with open(CSDinfo_filename, 'w') as outfile:\n",
    "        json.dump(LFPtraces_info, outfile, indent = 4, separators = (',', ': '))\n",
    "    print('  Saving {}.'.format(CSDtime_filename))\n",
    "    np.save(CSDtime_filename, lfp_event_ts, allow_pickle=False)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on single subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_folder = r'F:\\EEG_exp\\mouse569073\\estim_vis_2021-04-15_10-27-22\\experiment1\\recording1'\n",
    "exp = EEGexp(rec_folder, preprocess=False, make_stim_csv=False)\n",
    "    \n",
    "exp_tag = exp.experiment_folder[exp.experiment_folder.find('mouse')+12:exp.experiment_folder.find(str(exp.date.year))-1]\n",
    "print(exp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a folder in my OneDrive to save plots\n",
    "plotsavedir = os.path.join(r'C:\\Users\\lesliec\\OneDrive - Allen Institute\\data\\plots', 'mouse' + exp.mouse)\n",
    "if not os.path.exists(plotsavedir):\n",
    "    os.mkdir(plotsavedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load stim table and speed signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stim_log = pd.read_csv(exp.stimulus_log_file)\n",
    "stim_log = stim_log.astype({'parameter': str})\n",
    "stim_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load unit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_list = [x.replace('_sorted', '') for x in exp.experiment_data if 'probe' in x]\n",
    "print(probe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_LFP_data = {}\n",
    "for probei in probe_list:\n",
    "    print(probei)\n",
    "    probe_LFP_data[probei] = {}\n",
    "    ## Load probe_info.json ##\n",
    "    with open(exp.ephys_params[probei]['probe_info']) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    npx_allch = np.array(data['channel']) # this is an array from 0 to 384\n",
    "    surface_ch = int(data['surface_channel']) # the electrode we said was at the brain surface\n",
    "    air_ch = int(data['air_channel']) # the electrode at the ACSF/air border\n",
    "    allch_z = np.array(data['vertical_pos']) # vertical pos of each elec (um), rel to the tip (ch 0 is 20 um from tip)\n",
    "    ref_mask = np.array(data['mask']) # contains a False for Npx reference channels and \"bad chs\"\n",
    "    print(' surface channel: {:d}'.format(surface_ch))\n",
    "    \n",
    "    ## Get all ch depths ##\n",
    "    npx_chs = np.array([x for x in npx_allch if ref_mask[x] and x <= surface_ch])\n",
    "    probe_LFP_data[probei]['ch_depths'] = allch_z[surface_ch] - allch_z\n",
    "    probe_LFP_data[probei]['all_chs'] = npx_allch\n",
    "    \n",
    "    ## Get saline chs for re-ref ##\n",
    "    if air_ch - 10 > surface_ch:\n",
    "        probe_LFP_data[probei]['saline_chs'] = np.arange(air_ch - 10, air_ch)\n",
    "    else:\n",
    "        probe_LFP_data[probei]['saline_chs'] = np.arange(surface_ch + 1, air_ch)\n",
    "    if 'area_ch' in data.keys():\n",
    "        probe_LFP_data[probei]['ch_areas'] = np.array(data['area_ch'])\n",
    "        probe_LFP_data[probei]['pop_chs'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "        probe_LFP_data[probei]['pop_ch_depths'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "        probe_LFP_data[probei]['pop_ch_areas'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "        probe_LFP_data[probei]['pop_ch_mask'] = {key: [] for key in list(areas_of_interest.keys())}\n",
    "        for chind in npx_allch: ## Now this will look at all chs, not excluding ACSF or bad chs ##\n",
    "            parent_region = [\n",
    "                k for k in list(areas_of_interest.keys()) if probe_LFP_data[probei]['ch_areas'][chind] in areas_of_interest[k]\n",
    "            ]\n",
    "            if len(parent_region) == 1:\n",
    "                probe_LFP_data[probei]['pop_chs'][parent_region[0]].append(chind)\n",
    "                probe_LFP_data[probei]['pop_ch_depths'][parent_region[0]].append(probe_LFP_data[probei]['ch_depths'][chind])\n",
    "                probe_LFP_data[probei]['pop_ch_areas'][parent_region[0]].append(probe_LFP_data[probei]['ch_areas'][chind])\n",
    "                probe_LFP_data[probei]['pop_ch_mask'][parent_region[0]].append(ref_mask[chind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get evoked LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_event = 2. # s, look at 2 s pre-stim\n",
    "after_event = 2. # s, look at 2 s post-stim\n",
    "\n",
    "apply_mask = True\n",
    "subsampling_factor = 5 # default = 2, can set as int value or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probei = 'probeB'\n",
    "\n",
    "## Memmap LFP data ##\n",
    "start = time.time()\n",
    "lfp_ts = np.load(exp.ephys_params[probei]['lfp_timestamps'])\n",
    "lfp_mm = np.memmap(\n",
    "    exp.ephys_params[probei]['lfp_continuous'], dtype='int16',\n",
    "    shape=(lfp_ts.size, exp.ephys_params[probei]['num_chs']), mode='r')\n",
    "end = time.time()\n",
    "print('time to memmap cont data: {:.2f} min'.format((end-start)/60))\n",
    "\n",
    "## Mask artifact ## loads to memory\n",
    "if apply_mask:\n",
    "    start = time.time()\n",
    "    mask_samples = int(0.002 * exp.ephys_params[probei]['lfp_sample_rate'])\n",
    "    lfpdata = lfp_mm.copy()\n",
    "    for etime in stim_log.loc[stim_log['stim_type'] == 'biphasic', 'onset'].to_numpy():\n",
    "        val = find_nearest_ind(lfp_ts, etime)\n",
    "        lfpdata[val:val+mask_samples, :] = lfpdata[val:val-mask_samples:-1, :]\n",
    "    end = time.time()\n",
    "    print('time to mask artifact: {:.2f} min'.format((end-start)/60))\n",
    "else:\n",
    "    lfpdata = lfp_mm.copy()\n",
    "del lfp_mm\n",
    "\n",
    "## Downsample timestamps and data ## reduces memory requirement if downsampled\n",
    "if subsampling_factor:\n",
    "    start = time.time()\n",
    "    lfp_ts = lfp_ts[::subsampling_factor]\n",
    "    new_samp_rate = exp.ephys_params[probei]['lfp_sample_rate'] / subsampling_factor\n",
    "    lfpdata = subsample_lfp(lfpdata, probe_LFP_data[probei]['all_chs'], subsampling_factor)\n",
    "    end = time.time()\n",
    "    print('time to downsample: {:.2f} min'.format((end-start)/60))\n",
    "else:\n",
    "    new_samp_rate = exp.ephys_params[probei]['lfp_sample_rate']\n",
    "\n",
    "## Remove DC offset ##\n",
    "start = time.time()\n",
    "lfpdata = remove_lfp_offset(lfpdata, new_samp_rate, 0.1, 1)\n",
    "end = time.time()\n",
    "print('time to highpass filter: {:.2f} min'.format((end-start)/60))\n",
    "\n",
    "## Subtract median of ACSF chs ##\n",
    "start = time.time()\n",
    "clean_lfp = np.zeros(lfpdata.shape, dtype='int16')\n",
    "saline_ref = np.median(lfpdata[:, probe_LFP_data[probei]['saline_chs']], axis=1)\n",
    "for chi in probe_LFP_data[probei]['all_chs']:\n",
    "    tmp = lfpdata[:, chi] - saline_ref\n",
    "    clean_lfp[:, chi] = tmp.astype('int16')\n",
    "del lfpdata\n",
    "end = time.time()\n",
    "print('time to re-reference: {:.2f} min'.format((end-start)/60))\n",
    "\n",
    "## Get evoked LFP traces ## increases amount of data in memory\n",
    "start = time.time()\n",
    "lfp_event_traces, lfp_event_ts = get_evoked_traces(\n",
    "    clean_lfp, lfp_ts, stim_log['onset'].values, before_event, after_event, new_samp_rate)\n",
    "lfp_event_traces = lfp_event_traces * exp.ephys_params[probei]['bit_volts']\n",
    "del lfp_ts, clean_lfp\n",
    "end = time.time()\n",
    "print('time to epoch LFP: {:.2f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_event_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_range = [-2, -0.01]\n",
    "tempdata = lfp_event_traces\n",
    "LFPts = lfp_event_ts\n",
    "\n",
    "BLinds = [find_nearest_ind(LFPts, baseline_range[0]), find_nearest_ind(LFPts, baseline_range[1])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create buffer above and below the target area's chs for spatial smoothing ##\n",
    "## Do this for the whole probe, not for each area ##\n",
    "tmpBoundaryStart = tempdata[:,0,:]\n",
    "BoundaryStart = np.repeat(tmpBoundaryStart[:,np.newaxis,:], 14, axis=1)\n",
    "tmpBoundaryEnd = tempdata[:,-1,:]\n",
    "BoundaryEnd = np.repeat(tmpBoundaryEnd[:,np.newaxis,:], 14, axis=1)\n",
    "tempdata = np.concatenate((BoundaryStart, tempdata, BoundaryEnd), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_event_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline correct the LFP ##\n",
    "tmpBaselineCorr = np.mean(tempdata[BLinds[0]:BLinds[1],:,:], axis=0)\n",
    "tempdata = tempdata - tmpBaselineCorr[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create buffer before and after the epoch window for temporal smoothing ##\n",
    "tmpBoundaryStart = tempdata[0,:,:]\n",
    "BoundaryStart = np.repeat(tmpBoundaryStart[np.newaxis,:,:], 2, axis=0)\n",
    "tmpBoundaryEnd = tempdata[-1,:,:]\n",
    "BoundaryEnd = np.repeat(tmpBoundaryEnd[np.newaxis,:,:], 1, axis=0)\n",
    "tempdata = np.concatenate((BoundaryStart, tempdata, BoundaryEnd), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_event_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "## Remove bad LFP chs and interpolate data ##\n",
    "tempdata = AutocleanLFP(tempdata)\n",
    "end = time.time()\n",
    "print('time to auto-clean LFP: {:.2f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "## Perform spatial and temporal smoothing ##\n",
    "## This should output an array with original LFP dimensions ##\n",
    "smoothdata = np.zeros((lfp_event_traces.shape[0], lfp_event_traces.shape[1]+2, lfp_event_traces.shape[2]))\n",
    "for iTrial in range(0, tempdata.shape[2]):\n",
    "    tmpMatrix = tempdata[:,:,iTrial]\n",
    "    tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 24) # space, corrects for time mismatch due to multiplexing\n",
    "    tmpMatrix = moving_average_2dim(tmpMatrix, 4) # space, corrects for non-linear ch arrangement\n",
    "    tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 4) # temporal smoothing\n",
    "    smoothdata[:,:,iTrial] = tmpMatrix\n",
    "del tempdata\n",
    "end = time.time()\n",
    "print('time to smooth LFP in space and time: {:.2f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lfp_event_traces.shape)\n",
    "print(smoothdata.shape)\n",
    "print(tmpMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd derivative across chs ##\n",
    "CSDdata = np.diff(smoothdata, n=2, axis=1)\n",
    "del smoothdata\n",
    "## Another baseline correction ##\n",
    "tmpBaselineCorr = np.mean(CSDdata[BLinds[0]:BLinds[1],:,:], axis=0)\n",
    "CSDdata = CSDdata - tmpBaselineCorr[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSDdata.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(\n",
    "    CSDdata[:, :, 370].T, cmap='bwr', interpolation='none', aspect='auto', origin='lower',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_LFP = {key: [] for key in list(areas_of_interest.keys())}\n",
    "raw_traces = {}\n",
    "for probei, LFP_data in probe_LFP_data.items():\n",
    "    ## Memmap LFP data ##\n",
    "    print(probei)\n",
    "    start = time.time()\n",
    "    lfp_ts = np.load(exp.ephys_params[probei]['lfp_timestamps'])\n",
    "    lfp_mm = np.memmap(\n",
    "        exp.ephys_params[probei]['lfp_continuous'], dtype='int16',\n",
    "        shape=(lfp_ts.size, exp.ephys_params[probei]['num_chs']), mode='r')\n",
    "    \n",
    "    ## Mask artifact ##\n",
    "    if apply_mask:\n",
    "        mask_samples = int(0.002 * exp.ephys_params[probei]['lfp_sample_rate'])\n",
    "        lfpdata = lfp_mm.copy()\n",
    "        for etime in stim_log.loc[stim_log['stim_type'] == 'biphasic', 'onset'].to_numpy():\n",
    "            val = find_nearest_ind(lfp_ts, etime)\n",
    "            lfpdata[val:val+mask_samples, :] = lfpdata[val:val-mask_samples:-1, :]\n",
    "    else:\n",
    "        lfpdata = lfp_mm.copy()\n",
    "    del lfp_mm\n",
    "    \n",
    "    ## Downsample timestamps and data ##\n",
    "    lfp_ts = lfp_ts[::subsampling_factor]\n",
    "    new_samp_rate = exp.ephys_params[probei]['lfp_sample_rate'] / subsampling_factor\n",
    "    lfpdata = subsample_lfp(lfpdata, LFP_data['all_chs'], subsampling_factor)\n",
    "    \n",
    "    ## Remove DC offset ##\n",
    "    lfpdata = remove_lfp_offset(lfpdata, new_samp_rate, 0.1, 1)\n",
    "    \n",
    "    ## Subtract median of ACSF chs ##\n",
    "    clean_lfp = np.zeros(lfpdata.shape, dtype='int16')\n",
    "    saline_ref = np.median(lfpdata[:, LFP_data['saline_chs']], axis=1)\n",
    "    for chi in LFP_data['all_chs']:\n",
    "        tmp = lfpdata[:, chi] - saline_ref\n",
    "        clean_lfp[:, chi] = tmp.astype('int16')\n",
    "    del lfpdata\n",
    "    \n",
    "    ## Get evoked LFP traces ##\n",
    "    lfp_event_traces, lfp_event_ts = get_evoked_traces(\n",
    "        clean_lfp, lfp_ts, stim_log['onset'].values, before_event, after_event, new_samp_rate)\n",
    "    lfp_event_traces = lfp_event_traces * exp.ephys_params[probei]['bit_volts']\n",
    "    \n",
    "    raw_traces[probei] = [lfp_event_ts, lfp_event_traces]\n",
    "    for region in LFP_data['pop_chs'].keys():\n",
    "        pop_chs = LFP_data['pop_chs'][region]\n",
    "        if len(pop_chs) == 0:\n",
    "            continue\n",
    "        pop_LFP[region].append(lfp_event_traces[:, pop_chs, :])\n",
    "\n",
    "    del lfp_ts\n",
    "    del clean_lfp\n",
    "    del lfp_event_traces\n",
    "    del lfp_event_ts\n",
    "    \n",
    "    end = time.time()\n",
    "    print(' Time to pre-process and epoch: {:.2f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_ch_depths = {key: [] for key in list(areas_of_interest.keys())}\n",
    "pop_ch_areas = {key: [] for key in list(areas_of_interest.keys())}\n",
    "for probei in probe_LFP_data.keys():\n",
    "    for region in probe_LFP_data[probei]['pop_ch_depths'].keys():\n",
    "        pop_ch_depths[region].append(probe_LFP_data[probei]['pop_ch_depths'][region])\n",
    "        pop_ch_areas[region].append(probe_LFP_data[probei]['ch_areas'][probe_LFP_data[probei]['pop_chs'][region]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot population LFP traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triali = 11\n",
    "vlevel = 5000\n",
    "\n",
    "fig, axs = plt.subplots(len(areas_of_interest), 1, figsize=(8,8), sharex=True, constrained_layout=True)\n",
    "timex = raw_traces['probeB'][0]\n",
    "for rowi, region in enumerate(areas_of_interest):\n",
    "    if len(pop_LFP[region]) == 0:\n",
    "        continue\n",
    "        \n",
    "    ## Plot LFP ##\n",
    "    LFPtraces = np.concatenate(pop_LFP[region], axis=1)\n",
    "    ch_depth_sort = np.squeeze(np.argsort(np.concatenate(pop_ch_depths[region], axis=0)))[::-1]\n",
    "    axs[rowi].imshow(\n",
    "        LFPtraces[:, ch_depth_sort, triali].T, cmap='bwr', interpolation='none', aspect='auto', origin='lower',\n",
    "        vmin=-vlevel, vmax=vlevel, extent=[timex[0], timex[-1], 0, ch_depth_sort.shape[0]]\n",
    "    )\n",
    "    \n",
    "    axs[rowi].set_ylabel('{} LFP\\ndeep <---> sup.'.format(region))\n",
    "    \n",
    "axs[rowi].set_xlabel('Time from stim onset (s)')    \n",
    "plt.suptitle('Evoked LFP traces for a single trial\\n{}: trial {:d}'.format(exp.mouse, triali), fontsize=11)\n",
    "## Save ##\n",
    "figname = 'allPOP_LFPtraces_{}_trial{:d}-resting.png'.format(exp.mouse, triali)\n",
    "# fig.savefig(os.path.join(plotsavedir, figname), transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPtraces_info = {}\n",
    "for region in pop_LFP.keys():\n",
    "    print(region)\n",
    "    start = time.time()\n",
    "    LFP_filename = os.path.join(exp.data_folder, region + '_evokedLFPtraces.npy')\n",
    "    if len(pop_LFP[region]) == 0:\n",
    "        print(' No chs were found for this region, not saving data.\\n')\n",
    "        continue\n",
    "    LFPtraces_info[region] = {}\n",
    "    \n",
    "    reg_ch_depths = np.concatenate(pop_ch_depths[region])\n",
    "    reg_ch_areas = np.concatenate(pop_ch_areas[region])\n",
    "    LFP_traces = np.concatenate(pop_LFP[region], axis=1)\n",
    "    \n",
    "    CH_DEPTH_SORT = np.argsort(reg_ch_depths)\n",
    "    LFPtraces_info[region]['ch_depths'] = reg_ch_depths[CH_DEPTH_SORT].tolist()\n",
    "    LFPtraces_info[region]['ch_areas'] = reg_ch_areas[CH_DEPTH_SORT].tolist()\n",
    "    \n",
    "    print(' Saving {}.'.format(LFP_filename))\n",
    "#     np.save(LFP_filename, LFP_traces[:, CH_DEPTH_SORT, :], allow_pickle=False)\n",
    "    end = time.time()\n",
    "    print(' Time to save: {:.2f} s'.format(end-start))\n",
    "    print('')\n",
    "    \n",
    "LFPinfo_filename = os.path.join(exp.data_folder, 'evokedLFPinfo.json')\n",
    "with open(LFPinfo_filename, 'w') as outfile:\n",
    "    json.dump(LFPtraces_info, outfile, indent = 4, separators = (',', ': '))\n",
    "    \n",
    "LFPtime_filename = os.path.join(exp.data_folder, 'evokedLFPtimestamps.npy')\n",
    "np.save(LFPtime_filename, timex, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original functions from Simone"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def computeCSD_SingleTrial(currSessDF, BaselineRange = (-2,-0.01), MainFolder =  r'E:\\AllenInstitute\\Data'):\n",
    "    #Cut = 1500\n",
    "    #LFP_sRate = 2500\n",
    "    #TsLFP = np.linspace(-Cut,Cut, num = Cut*2)/LFP_sRate\n",
    "    currSessFolder = MainFolder + '\\\\' + currSessDF['mouse_name'].values[0] + '\\\\' + currSessDF['exp_name'].values[0] + '\\\\' + 'probe' +currSessDF['Npx'].values[0] + '\\\\' + str(int(currSessDF['sweep_num'].values[0])) + '_' + currSessDF['brain_states'].values[0] + '\\\\' + str(int(currSessDF['stim_int'].values[0])) + '\\\\'\n",
    "    TsLFP = np.load(currSessFolder + 'TsLFP.npy')\n",
    "    #print(TsLFP)\n",
    "    LFP = np.load(currSessFolder + 'SplittedLFP.npy')\n",
    "    BaselineIndices = (find_nearest_ind(TsLFP, BaselineRange[0]), find_nearest_ind(TsLFP, BaselineRange[1]))\n",
    "    \n",
    "    ## this is for the whole probe ##\n",
    "    tmpERP = LFP\n",
    "    tmpBoundaryStart = tmpERP[:,0,:]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[:,np.newaxis,:], 14, axis=1)\n",
    "    tmpBoundaryEnd = tmpERP[:,-1,:]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[:,np.newaxis,:], 14, axis=1)\n",
    "    tmpERP = np.concatenate((BoundaryStart,tmpERP,BoundaryEnd), axis =1)\n",
    "\n",
    "\n",
    "    ## adding timepoints at beginning and end for temporal smoothing ##\n",
    "    tmpBoundaryStart = tmpERP[0,:,:]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[np.newaxis,:,:], 2, axis=0)\n",
    "    tmpBoundaryEnd = tmpERP[-1,:,:]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[np.newaxis,:,:], 1, axis=0)\n",
    "    tmpERP = np.concatenate((BoundaryStart,tmpERP,BoundaryEnd), axis =0)\n",
    "    \n",
    "    ## baseline correction ##\n",
    "    for iTrials in range(0,tmpERP.shape[2]):\n",
    "        tmpBaselineCorr = np.mean(tmpERP[BaselineIndices[0]:BaselineIndices[1],:,iTrials], axis = 0)\n",
    "        tmpERP[:,:,iTrials] = tmpERP[:,:,iTrials] - tmpBaselineCorr[np.newaxis,:]\n",
    "    \n",
    "    tmpERP = AutocleanLFP(tmpERP)\n",
    "\n",
    "    tmpERPNew = np.zeros((tmpERP.shape[0]-3, tmpERP.shape[1]-26, tmpERP.shape[2]))\n",
    "    for iTrials in range(0,tmpERP.shape[2]):\n",
    "        tmpMatrix = tmpERP[:,:,iTrials]\n",
    "        tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 24) # space\n",
    "        tmpMatrix = moving_average_2dim(tmpMatrix, 4) # space\n",
    "        tmpMatrix = moving_average_2dim(np.transpose(tmpMatrix), 4) # time\n",
    "        tmpERPNew[:,:,iTrials] = tmpMatrix\n",
    "    \n",
    "    ## 2nd derivative ##\n",
    "    tmpERPNew = np.diff(tmpERPNew, n = 2, axis = 1)\n",
    "    \n",
    "    ## baseline correction ##\n",
    "    tmpBaselineCorr = np.mean(tmpERPNew[BaselineIndices[0]:BaselineIndices[1],:,:], axis = 0)\n",
    "    tmpERPNew = tmpERPNew - tmpBaselineCorr[np.newaxis,:,:]\n",
    "    \n",
    "    ## smoothing should remove extra samples ##\n",
    "    return tmpERPNew, TsLFP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def computeCSD_Evoked(currSessDF, BaselineRange = (-2,-0.01), MainFolder =  r'E:\\AllenInstitute\\Data'):\n",
    "    #Cut = 1500\n",
    "    #LFP_sRate = 2500\n",
    "    #TsLFP = np.linspace(-Cut,Cut, num = Cut*2)/LFP_sRate\n",
    "    currSessFolder = MainFolder + '\\\\' + currSessDF['mouse_name'].values[0] + '\\\\' + currSessDF['exp_name'].values[0] + '\\\\' + 'probe' +currSessDF['Npx'].values[0] + '\\\\' + str(int(currSessDF['sweep_num'].values[0])) + '_' + currSessDF['brain_states'].values[0] + '\\\\' + str(int(currSessDF['stim_int'].values[0])) + '\\\\'\n",
    "    TsLFP = np.load(currSessFolder + 'TsLFP.npy')\n",
    "    #print(TsLFP)\n",
    "    LFP = np.load(currSessFolder + 'EvokedLFP.npy')\n",
    "    BaselineIndices = (find_nearest_ind(TsLFP, BaselineRange[0]), find_nearest_ind(TsLFP, BaselineRange[1]))\n",
    "    \n",
    "    tmpERP = LFP\n",
    "    tmpBoundaryStart = tmpERP[:,0]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[:,np.newaxis], 14, axis=1)\n",
    "    tmpBoundaryEnd = tmpERP[:,-1]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[:,np.newaxis], 14, axis=1)\n",
    "    tmpERP = np.concatenate((BoundaryStart,tmpERP,BoundaryEnd), axis =1)\n",
    "\n",
    "\n",
    "\n",
    "    tmpBoundaryStart = tmpERP[0,:]\n",
    "    BoundaryStart = np.repeat(tmpBoundaryStart[np.newaxis,:], 2, axis=0)\n",
    "    tmpBoundaryEnd = tmpERP[-1,:]\n",
    "    BoundaryEnd = np.repeat(tmpBoundaryEnd[np.newaxis,:], 1, axis=0)\n",
    "    tmpERP = np.concatenate((BoundaryStart,tmpERP,BoundaryEnd), axis =0)\n",
    "    \n",
    "    tmpBaselineCorr = np.mean(tmpERP[BaselineIndices[0]:BaselineIndices[1],:], axis = 0)\n",
    "    tmpERP = tmpERP - np.expand_dims(tmpBaselineCorr, axis=0)\n",
    "    \n",
    "    tmpERP = AutocleanLFP(tmpERP)\n",
    "    \n",
    "    tmpERP = moving_average_2dim(np.transpose(tmpERP), 24)\n",
    "    tmpERP = moving_average_2dim(tmpERP, 4)\n",
    "    tmpERP = moving_average_2dim(np.transpose(tmpERP), 4)\n",
    "\n",
    "\n",
    "\n",
    "    tmpERP = - np.diff(tmpERP, n = 2, axis = 1)\n",
    "    tmpERP = tmpERP - np.mean(tmpERP[BaselineIndices[0]:BaselineIndices[1],:], axis = 0)\n",
    "    \n",
    "    return tmpERP, TsLFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbd_eeg",
   "language": "python",
   "name": "tbd_eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
